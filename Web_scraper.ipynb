{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "\n",
    "def webScraper():\n",
    "    \n",
    "    global results\n",
    "\n",
    "    results = []\n",
    "\n",
    "    pages=['1/500001','1/500011','1/500021','1/500041','1/500051','1/500061','1/500071','1/500081','1/500101','1/500105',\\\n",
    "'101/501171','101/501181','101/501185','101/501191','101/501195','101/501201','101/501211','101/501221','101/501241',\\\n",
    "'101/501251','11/500111','11/500121','11/500131','11/500141','11/500151','11/500161','11/500171','11/500181',\\\n",
    "'11/500191','11/500201','11/500211','11/500221','111/501261','111/501271','111/501291','111/501301','111/501311',\\\n",
    "'111/501321','111/501331','111/501341','111/501351','121/501361','121/501371','121/501381','121/501391','121/501401',\\\n",
    "'121/501411','121/501421','141/501501','141/501511','141/501521','141/501531','141/501541','141/501551','141/501561',\\\n",
    "'141/501571','151/501591','151/501601','151/501611','151/501621','151/501631','151/501641','151/501645','161/501651',\\\n",
    "'161/501661','161/501671','161/501681','161/501691','161/501701','161/501711','161/501731','161/501741','161/501751',\\\n",
    "'161/501761','161/501771','161/501781','171/501741','171/501801','171/501811','171/501821','171/501831','171/501861',\\\n",
    "'171/501865','171/501871','171/501881','171/501891','171/501895','171/501911','171/501921','171/501931','171/501941',\\\n",
    "'171/501951','171/501965','171/501971','171/502001','181/502011','181/502021','181/502031','181/502041','181/502051',\\\n",
    "'181/502061','181/502071','181/502081','181/502085','181/502091','181/502095','181/502101','181/502111','181/502121',\\\n",
    "'181/502131','181/502132','181/502135','181/502175','181/502211','191/502231','191/502241','191/502251','191/502271',\\\n",
    "'191/502281','21/500231','21/500241','21/500251','21/500261','21/500271','21/500291','21/500305','211/502351','211/502361',\\\n",
    "'211/502371','211/502381','211/502391','211/502411','211/502421','211/502431','211/502441','211/502451','211/502461',\\\n",
    "'211/502471','211/502481','211/502491','221/502501','221/502511','221/502521','221/502531','221/502541','221/502551',\\\n",
    "'221/502561','221/502571','221/502581','231/502601','231/502611','231/502621','231/502631','231/502641','231/502661',\\\n",
    "'231/502671','231/502685','241/502711','241/502721','241/502731','241/502751','241/502771','251/502781','251/502785',\\\n",
    "'251/502851','251/502853','251/502855','251/502871','31/500321','31/500331','31/500341','31/500351','31/500361',\\\n",
    "'31/500371','31/500381','41/500401','41/500421','41/500431','41/500441','41/500451','41/500461','41/500471','41/500481',\\\n",
    "'51/500501','51/500511','51/500513','61/500531','61/500551','61/500561','71/500571','71/500581','71/500591','71/500611',\\\n",
    "'71/500621','71/500625','71/500631','71/500641','71/500651','71/500661','71/500671','71/500681','81/500721','81/500901',\\\n",
    "'81/500911','81/500921','81/500931','81/500951','83/500821','83/500831','85/500851','85/500861','85/500881','85/500891',\\\n",
    "'85/500895','87/500731','87/500735','87/500741','88/500711','88/500761','88/500801','88/500811','88/500871','89/500771',\\\n",
    "'89/500781','89/500791','89/500941','91/500961','91/500971','91/500981','91/501001','91/501011','91/501021','91/501051',\\\n",
    "'91/501061','91/501071','91/501081','91/501085','91/501091','91/501105','91/501121','91/501141','91/501145','91/501151']\n",
    "\n",
    "\n",
    "    userAgent = UserAgent()\n",
    "    \n",
    "    reqNR = 1\n",
    "    \n",
    "    reqToRun = range(29,500,29)\n",
    "    \n",
    "    global invPages\n",
    "    invPages = []\n",
    "\n",
    "    for page in pages:\n",
    "        if reqNR in reqToRun:\n",
    "            print('waiting 1.5 minutes in order to AVOID DETECTION...\\n')\n",
    "            time.sleep(90)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        url = f\"https://www.***.co.uk/grocery/{page}\"\n",
    "        print(f'getting page {url}...\\nrequest {reqNR}')\n",
    "        reqNR+=1\n",
    "        req = requests.get(url,headers={'user-agent':str(userAgent.chrome)})\n",
    "        soup = bs(req.content, \"lxml\")\n",
    "        \n",
    "        if soup.select('h2.prodname'):\n",
    "            results.append(soup)\n",
    "            print('page SAVED!\\n')\n",
    "        else:\n",
    "            invPages.append(url)\n",
    "            print('page does not exist!')\n",
    "            print('moving on...\\n')\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        nr = range(20,320,20)\n",
    "        \n",
    "        for n in nr:\n",
    "            if reqNR in reqToRun:\n",
    "                print('waiting 1.5 minutes in order to AVOID DETECTION...\\n')\n",
    "                time.sleep(90)\n",
    "            else:\n",
    "                pass\n",
    "            p = '?s='+str(n)\n",
    "            url = f\"https://www.***.co.uk/grocery/{page}{p}\"\n",
    "            print(f'getting page {url}...\\nrequest {reqNR}')\n",
    "            reqNR+=1\n",
    "            req1 = requests.get(url,headers={'user-agent':str(userAgent.chrome)})\n",
    "            soup1 = bs(req1.content, \"lxml\")\n",
    "            \n",
    "            if soup1.select('h2.prodname'):\n",
    "                results.append(soup1)\n",
    "                print('page SAVED!\\n')                                            \n",
    "            else:\n",
    "                invPages.append(url)\n",
    "                print('page does not exist!')\n",
    "                print('moving on...\\n')\n",
    "                break\n",
    "\n",
    "# Count how long it takes to run the script\n",
    "print(f'--------------------------------------------------\\n\\\n",
    "It has taken {timeit.timeit(webScraper,number=1)} seconds to get the pages\\n')\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "print(f'These pages ARE NOT VALID:\\n')\n",
    "print(f'{invPages}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = []\n",
    "pn = []\n",
    "ps = []\n",
    "pp = []\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "print(f'Extracting the relevant data from the SAVED PAGES...\\n')\n",
    "\n",
    "for item in results:\n",
    "    for img in item.select('div.prodimage img'):\n",
    "        img = str(img)\n",
    "        if 'data-src' in img:\n",
    "            img = img.replace('data-src','src')\n",
    "            pi.append(img)\n",
    "        else:\n",
    "            pi.append(img)\n",
    "            \n",
    "    for n in item.select('h2.prodname'):\n",
    "        pn.append(n.get_text())\n",
    "        \n",
    "    for s in item.select('p.prodsize'):\n",
    "        ps.append(s.get_text())\n",
    "        \n",
    "    for p in item.select('p.prodrsp'):\n",
    "        pp.append(p.get_text()[6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------------')\n",
    "print(f'Creating the pandas Data Frames...\\n')\n",
    "\n",
    "pn = pd.Series(pn,name='name')\n",
    "ps = pd.Series(ps,name='pack')\n",
    "pp = pd.Series(pp,name='price')\n",
    "\n",
    "df = pd.DataFrame(pd.Series(pi,name='image'))\n",
    "df['name'] = pn\n",
    "df['pack'] = ps\n",
    "df['price'] = pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------------')\n",
    "print('Exporting the pandas Data Frames as \"products.html\" file...\\n')\n",
    "\n",
    "df.to_html('products.html',escape=False)\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "print('Adding CSS styling to products.html file...\\n')\n",
    "\n",
    "text = open('products.html','r').read()\n",
    "\n",
    "text = '<head><style>table {border-collapse:collapse;text-align:center;} tr:nth-child(even) {background:#e5e5e5}\\\n",
    "       th {text-align:center} img {width:250px;height:250px;}</style></head>\\n' + text\n",
    "\n",
    "with open('products.html','w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "print('--------------------------------------------------')\n",
    "print('DONE. CHECK THE \"products.html\" FILE!\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
